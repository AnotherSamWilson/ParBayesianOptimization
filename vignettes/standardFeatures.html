<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="viewport" content="width=device-width, initial-scale=1">

<meta name="author" content="Samuel Wilson" />

<meta name="date" content="2018-11-15" />

<title>Introduction to ParBayesianOptimization</title>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>



<link href="data:text/css;charset=utf-8,body%20%7B%0Abackground%2Dcolor%3A%20%23fff%3B%0Amargin%3A%201em%20auto%3B%0Amax%2Dwidth%3A%20700px%3B%0Aoverflow%3A%20visible%3B%0Apadding%2Dleft%3A%202em%3B%0Apadding%2Dright%3A%202em%3B%0Afont%2Dfamily%3A%20%22Open%20Sans%22%2C%20%22Helvetica%20Neue%22%2C%20Helvetica%2C%20Arial%2C%20sans%2Dserif%3B%0Afont%2Dsize%3A%2014px%3B%0Aline%2Dheight%3A%201%2E35%3B%0A%7D%0A%23header%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0A%23TOC%20%7B%0Aclear%3A%20both%3B%0Amargin%3A%200%200%2010px%2010px%3B%0Apadding%3A%204px%3B%0Awidth%3A%20400px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Aborder%2Dradius%3A%205px%3B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Afont%2Dsize%3A%2013px%3B%0Aline%2Dheight%3A%201%2E3%3B%0A%7D%0A%23TOC%20%2Etoctitle%20%7B%0Afont%2Dweight%3A%20bold%3B%0Afont%2Dsize%3A%2015px%3B%0Amargin%2Dleft%3A%205px%3B%0A%7D%0A%23TOC%20ul%20%7B%0Apadding%2Dleft%3A%2040px%3B%0Amargin%2Dleft%3A%20%2D1%2E5em%3B%0Amargin%2Dtop%3A%205px%3B%0Amargin%2Dbottom%3A%205px%3B%0A%7D%0A%23TOC%20ul%20ul%20%7B%0Amargin%2Dleft%3A%20%2D2em%3B%0A%7D%0A%23TOC%20li%20%7B%0Aline%2Dheight%3A%2016px%3B%0A%7D%0Atable%20%7B%0Amargin%3A%201em%20auto%3B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dcolor%3A%20%23DDDDDD%3B%0Aborder%2Dstyle%3A%20outset%3B%0Aborder%2Dcollapse%3A%20collapse%3B%0A%7D%0Atable%20th%20%7B%0Aborder%2Dwidth%3A%202px%3B%0Apadding%3A%205px%3B%0Aborder%2Dstyle%3A%20inset%3B%0A%7D%0Atable%20td%20%7B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dstyle%3A%20inset%3B%0Aline%2Dheight%3A%2018px%3B%0Apadding%3A%205px%205px%3B%0A%7D%0Atable%2C%20table%20th%2C%20table%20td%20%7B%0Aborder%2Dleft%2Dstyle%3A%20none%3B%0Aborder%2Dright%2Dstyle%3A%20none%3B%0A%7D%0Atable%20thead%2C%20table%20tr%2Eeven%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Ap%20%7B%0Amargin%3A%200%2E5em%200%3B%0A%7D%0Ablockquote%20%7B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Apadding%3A%200%2E25em%200%2E75em%3B%0A%7D%0Ahr%20%7B%0Aborder%2Dstyle%3A%20solid%3B%0Aborder%3A%20none%3B%0Aborder%2Dtop%3A%201px%20solid%20%23777%3B%0Amargin%3A%2028px%200%3B%0A%7D%0Adl%20%7B%0Amargin%2Dleft%3A%200%3B%0A%7D%0Adl%20dd%20%7B%0Amargin%2Dbottom%3A%2013px%3B%0Amargin%2Dleft%3A%2013px%3B%0A%7D%0Adl%20dt%20%7B%0Afont%2Dweight%3A%20bold%3B%0A%7D%0Aul%20%7B%0Amargin%2Dtop%3A%200%3B%0A%7D%0Aul%20li%20%7B%0Alist%2Dstyle%3A%20circle%20outside%3B%0A%7D%0Aul%20ul%20%7B%0Amargin%2Dbottom%3A%200%3B%0A%7D%0Apre%2C%20code%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0Aborder%2Dradius%3A%203px%3B%0Acolor%3A%20%23333%3B%0Awhite%2Dspace%3A%20pre%2Dwrap%3B%20%0A%7D%0Apre%20%7B%0Aborder%2Dradius%3A%203px%3B%0Amargin%3A%205px%200px%2010px%200px%3B%0Apadding%3A%2010px%3B%0A%7D%0Apre%3Anot%28%5Bclass%5D%29%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Acode%20%7B%0Afont%2Dfamily%3A%20Consolas%2C%20Monaco%2C%20%27Courier%20New%27%2C%20monospace%3B%0Afont%2Dsize%3A%2085%25%3B%0A%7D%0Ap%20%3E%20code%2C%20li%20%3E%20code%20%7B%0Apadding%3A%202px%200px%3B%0A%7D%0Adiv%2Efigure%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0Aimg%20%7B%0Abackground%2Dcolor%3A%20%23FFFFFF%3B%0Apadding%3A%202px%3B%0Aborder%3A%201px%20solid%20%23DDDDDD%3B%0Aborder%2Dradius%3A%203px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Amargin%3A%200%205px%3B%0A%7D%0Ah1%20%7B%0Amargin%2Dtop%3A%200%3B%0Afont%2Dsize%3A%2035px%3B%0Aline%2Dheight%3A%2040px%3B%0A%7D%0Ah2%20%7B%0Aborder%2Dbottom%3A%204px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Apadding%2Dbottom%3A%202px%3B%0Afont%2Dsize%3A%20145%25%3B%0A%7D%0Ah3%20%7B%0Aborder%2Dbottom%3A%202px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Afont%2Dsize%3A%20120%25%3B%0A%7D%0Ah4%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23f7f7f7%3B%0Amargin%2Dleft%3A%208px%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Ah5%2C%20h6%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23ccc%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Aa%20%7B%0Acolor%3A%20%230033dd%3B%0Atext%2Ddecoration%3A%20none%3B%0A%7D%0Aa%3Ahover%20%7B%0Acolor%3A%20%236666ff%3B%20%7D%0Aa%3Avisited%20%7B%0Acolor%3A%20%23800080%3B%20%7D%0Aa%3Avisited%3Ahover%20%7B%0Acolor%3A%20%23BB00BB%3B%20%7D%0Aa%5Bhref%5E%3D%22http%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0Aa%5Bhref%5E%3D%22https%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0A%0Acode%20%3E%20span%2Ekw%20%7B%20color%3A%20%23555%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Edt%20%7B%20color%3A%20%23902000%3B%20%7D%20%0Acode%20%3E%20span%2Edv%20%7B%20color%3A%20%2340a070%3B%20%7D%20%0Acode%20%3E%20span%2Ebn%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Efl%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Ech%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Est%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Eco%20%7B%20color%3A%20%23888888%3B%20font%2Dstyle%3A%20italic%3B%20%7D%20%0Acode%20%3E%20span%2Eot%20%7B%20color%3A%20%23007020%3B%20%7D%20%0Acode%20%3E%20span%2Eal%20%7B%20color%3A%20%23ff0000%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Efu%20%7B%20color%3A%20%23900%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%20code%20%3E%20span%2Eer%20%7B%20color%3A%20%23a61717%3B%20background%2Dcolor%3A%20%23e3d2d2%3B%20%7D%20%0A" rel="stylesheet" type="text/css" />

</head>

<body>




<h1 class="title toc-ignore">Introduction to ParBayesianOptimization</h1>
<h4 class="author"><em>Samuel Wilson</em></h4>
<h4 class="date"><em>2018-11-15</em></h4>



<hr />
<div id="package-process" class="section level2">
<h2>Package Process</h2>
<p>Machine learning projects will commonly require a user to “tune” a model’s hyperparameters to find a good balance between bias and variance. Several tools are available in a data scientist’s toolbox to handle this task, the most blunt of which is a grid search. A grid search gauges the model performance over a pre-defined set of hyperparameters without regard for past performance. As models increase in complexity and training time, grid searches become unwieldly.</p>
<p>Idealy, we would use the information from prior model evaluations to guide us in our future parameter searches. This is precisely the idea behind Bayesian Optimization, in which our prior response distribution is iteratively updated based on our best guess of where the best parameters are. The <code>ParBayesianOptimization</code> package does exactly this in the following process:</p>
<ol style="list-style-type: decimal">
<li>Initial parameter-score pairs are found<br />
</li>
<li>Gaussian Process is fit/updated</li>
<li>Numerical methods are used to estimate the best parameter set<br />
</li>
<li>New parameter-score pairs are found<br />
</li>
<li>Repeat steps 2-4 until some stopping criteria is met</li>
</ol>
<hr />
</div>
<div id="practical-example" class="section level2">
<h2>Practical Example</h2>
<p>In this example, we will be using the agaricus.train dataset provided in the XGBoost package. Here, we load the packages, data, and create a folds object to be used in the scoring function.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(<span class="st">&quot;xgboost&quot;</span>)
<span class="kw">library</span>(<span class="st">&quot;ParBayesianOptimization&quot;</span>)
<span class="co">#&gt; Loading required package: data.table</span>
<span class="co">#&gt; Loading required package: GauPro</span>
<span class="co">#&gt; Loading required package: foreach</span>
<span class="co">#&gt; Loading required package: dbscan</span>

<span class="kw">data</span>(agaricus.train, <span class="dt">package =</span> <span class="st">&quot;xgboost&quot;</span>)

Folds &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">Fold1 =</span> <span class="kw">as.integer</span>(<span class="kw">seq</span>(<span class="dv">1</span>,<span class="kw">nrow</span>(agaricus.train<span class="op">$</span>data),<span class="dt">by =</span> <span class="dv">3</span>))
            , <span class="dt">Fold2 =</span> <span class="kw">as.integer</span>(<span class="kw">seq</span>(<span class="dv">2</span>,<span class="kw">nrow</span>(agaricus.train<span class="op">$</span>data),<span class="dt">by =</span> <span class="dv">3</span>))
            , <span class="dt">Fold3 =</span> <span class="kw">as.integer</span>(<span class="kw">seq</span>(<span class="dv">3</span>,<span class="kw">nrow</span>(agaricus.train<span class="op">$</span>data),<span class="dt">by =</span> <span class="dv">3</span>)))</code></pre></div>
<p>Now we need to define the scoring function. This function should, at a minimum, return a list with a <code>Score</code> element, which is the model evaluation metric we want to maximize. We can also retain other pieces of information created by the scoring function by including them as named elements of the returned list. In this case, we want to retain the optimal number of rounds determined by the <code>xgb.cv</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">scoringFunction &lt;-<span class="st"> </span><span class="cf">function</span>(max_depth, min_child_weight, subsample) {

  dtrain &lt;-<span class="st"> </span><span class="kw">xgb.DMatrix</span>(agaricus.train<span class="op">$</span>data,<span class="dt">label =</span> agaricus.train<span class="op">$</span>label)
  
  Pars &lt;-<span class="st"> </span><span class="kw">list</span>( <span class="dt">booster =</span> <span class="st">&quot;gbtree&quot;</span>
              , <span class="dt">eta =</span> <span class="fl">0.01</span>
              , <span class="dt">max_depth =</span> max_depth
              , <span class="dt">min_child_weight =</span> min_child_weight
              , <span class="dt">subsample =</span> subsample
              , <span class="dt">objective =</span> <span class="st">&quot;binary:logistic&quot;</span>
              , <span class="dt">eval_metric =</span> <span class="st">&quot;auc&quot;</span>)

  xgbcv &lt;-<span class="st"> </span><span class="kw">xgb.cv</span>(<span class="dt">params =</span> Pars
                , <span class="dt">data =</span> dtrain
                , <span class="dt">nround =</span> <span class="dv">100</span>
                , <span class="dt">folds =</span> Folds
                , <span class="dt">prediction =</span> <span class="ot">TRUE</span>
                , <span class="dt">showsd =</span> <span class="ot">TRUE</span>
                , <span class="dt">early_stopping_rounds =</span> <span class="dv">5</span>
                , <span class="dt">maximize =</span> <span class="ot">TRUE</span>
                , <span class="dt">verbose =</span> <span class="dv">0</span>)

  <span class="kw">return</span>(<span class="kw">list</span>( <span class="dt">Score =</span> <span class="kw">max</span>(xgbcv<span class="op">$</span>evaluation_log<span class="op">$</span>test_auc_mean)
             , <span class="dt">nrounds =</span> xgbcv<span class="op">$</span>best_iteration
             )
         )
}</code></pre></div>
<p>Some other objects we need to define are the bounds, GP kernel and acquisition function.</p>
<ul>
<li>The <code>bounds</code> will tell our process its search space.</li>
<li>The kernel is passed to the <code>GauPro</code> function <code>GauPro_kernel_model</code> and defines the covariance function.</li>
<li>The acquisition function defines the utility we get from using a certain parameter set.</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">bounds &lt;-<span class="st"> </span><span class="kw">list</span>( <span class="dt">max_depth =</span> <span class="kw">c</span>(2L, 10L)
              , <span class="dt">min_child_weight =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">100</span>)
              , <span class="dt">subsample =</span> <span class="kw">c</span>(<span class="fl">0.25</span>, <span class="dv">1</span>))

kern &lt;-<span class="st"> &quot;Matern52&quot;</span>

acq &lt;-<span class="st"> &quot;ei&quot;</span></code></pre></div>
<p>We are now ready to put this all into the <code>BayesianOptimization</code> function.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ScoreResult &lt;-<span class="st"> </span><span class="kw">BayesianOptimization</span>(<span class="dt">FUN =</span> scoringFunction
                                  , <span class="dt">bounds =</span> bounds
                                  , <span class="dt">initPoints =</span> <span class="dv">10</span>
                                  , <span class="dt">bulkNew =</span> <span class="dv">1</span>
                                  , <span class="dt">nIters =</span> <span class="dv">12</span>
                                  , <span class="dt">kern =</span> kern
                                  , <span class="dt">acq =</span> acq
                                  , <span class="dt">kappa =</span> <span class="fl">2.576</span>
                                  , <span class="dt">gsPoints =</span> <span class="dv">200</span>
                                  , <span class="dt">verbose =</span> <span class="dv">1</span>
                                  , <span class="dt">parallel =</span> <span class="ot">FALSE</span>)
<span class="co">#&gt; </span>
<span class="co">#&gt; Running initial scoring function 10 times in 1 thread(s).</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Starting round number 1</span>
<span class="co">#&gt;   1) Fitting Gaussian process...</span>
<span class="co">#&gt;   2) Running local optimum search...</span>
<span class="co">#&gt;   3) Running scoring function 1 times in 1 thread(s)...</span>
<span class="co">#&gt; </span>
<span class="co">#&gt; Starting round number 2</span>
<span class="co">#&gt;   1) Fitting Gaussian process...</span>
<span class="co">#&gt;   2) Running local optimum search...</span>
<span class="co">#&gt;   3) Running scoring function 1 times in 1 thread(s)...</span></code></pre></div>
<p>The console informs us that the process initialized by running <code>scoringFunction</code> 10 times. It then fit a Gaussian process to the parameter-score pairs, found the global optimum of the acquisition function, and ran <code>scoringFunction</code> again. This process continued until we had 12 parameter-score pairs. You can interrogate the <code>ScoreResult</code> object to see the results:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ScoreResult<span class="op">$</span>ScoreDT
<span class="co">#&gt;     Iteration max_depth min_child_weight subsample Elapsed     Score nrounds</span>
<span class="co">#&gt;  1:         0         7        69.779582 0.7336753    0.14 0.9779723       1</span>
<span class="co">#&gt;  2:         0         9        55.269856 0.6517663    0.31 0.9835217       8</span>
<span class="co">#&gt;  3:         0         9        27.557603 0.4428883    0.52 0.9928417      20</span>
<span class="co">#&gt;  4:         0         3        20.788160 0.3791343    0.13 0.9892793       1</span>
<span class="co">#&gt;  5:         0         8        48.962160 0.7818707    0.19 0.9881603       3</span>
<span class="co">#&gt;  6:         0         8        57.418718 0.9439246    0.28 0.9881503       7</span>
<span class="co">#&gt;  7:         0         5        38.560753 0.6203772    0.23 0.9897887       5</span>
<span class="co">#&gt;  8:         0         9        24.774254 0.9070661    0.66 0.9968733      20</span>
<span class="co">#&gt;  9:         0        10        57.906844 0.8786701    0.22 0.9877853       4</span>
<span class="co">#&gt; 10:         0         9         7.184887 0.6849947    1.58 0.9997660      55</span>
<span class="co">#&gt; 11:         1         5         1.000000 1.0000000    0.20 0.9970903       1</span>
<span class="co">#&gt; 12:         2         8         1.000000 0.6734338    0.22 0.9984757       1</span></code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ScoreResult<span class="op">$</span>BestPars
<span class="co">#&gt;    Iteration max_depth min_child_weight subsample    Score nrounds elapsedSecs</span>
<span class="co">#&gt; 1:         0         9         7.184887 0.6849947 0.999766      55      5 secs</span>
<span class="co">#&gt; 2:         1         9         7.184887 0.6849947 0.999766      55     12 secs</span>
<span class="co">#&gt; 3:         2         9         7.184887 0.6849947 0.999766      55     31 secs</span></code></pre></div>
</div>



<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
